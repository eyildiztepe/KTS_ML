# Veri Seti {#veri_seti}
```{r echo=FALSE,warning=FALSE,message=FALSE}
if(!require(reticulate)) install.packages("reticulate", repos = "http://cran.rstudio.com")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.rstudio.com")
if(!require(caret)) install.packages("caret",repos = "http://cran.rstudio.com")
if(!require(caretEnsemble))  install.packages("caretEnsemble",repos = "http://cran.rstudio.com")
if(!require(doParallel))  install.packages("doParallel",repos = "http://cran.rstudio.com")
if(!require(data.table))  install.packages("data.table",repos = "http://cran.rstudio.com")
if(!require(dplyr))  install.packages("dplyr",repos = "http://cran.rstudio.com")
if(!require(e1071))  install.packages("e1071",repos = "http://cran.rstudio.com")
#if(!require(gbm))  install.packages("gbm",repos = "http://cran.rstudio.com")
if(!require(kernlab))  install.packages("kernlab",repos = "http://cran.rstudio.com")
#if(!require(randomForest))  install.packages("randomForest",repos = "http://cran.rstudio.com")
if(!require(tidyverse))  install.packages("tidyverse",repos = "http://cran.rstudio.com")
#if(!require(xgboost))  install.packages("xgboost",repos = "http://cran.rstudio.com")
if(!require(smotefamily))  install.packages("smotefamily",repos = "http://cran.rstudio.com")
```
```{python,echo=FALSE}
import numpy as np, pandas as pd, matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import f_oneway, chi2_contingency
CTS = pd.read_csv("data/CTS.csv",sep=",")
dataGroup = CTS[["Severity","Age","BMI","CSA","PB","Duration","NRS"]]
dataOverall = CTS[["Age","BMI","CSA","PB","Duration","NRS"]]
meanoval, stdoval = round(dataOverall.mean(),1), round(dataOverall.std(ddof=1),1)
means = round(dataGroup.groupby("Severity").mean(),1)
stds = round(dataGroup.groupby("Severity").std(ddof=1),1)
#####
mild = CTS[CTS.Severity == "mild"]
moderate = CTS[CTS.Severity == "moderate"]
severe = CTS[CTS.Severity == "severe"]
numVar = ["Age","BMI","CSA","PB","Duration","NRS"]
catVar = ["Sex","Side","Diabetes","NP","Weakness"]
p_values = []
p_vals2 = []
for i in numVar:
    _,p_val = f_oneway(mild[i],moderate[i],severe[i])
    p_values.append(round(p_val,3))
for i in catVar:
    var_0 = np.array([sum(mild[i] == 0),sum(moderate[i] == 0),sum(severe[i] == 0)])
    var_1 = np.array([sum(mild[i] == 1),sum(moderate[i] == 1),sum(severe[i] == 1)])
    p_vals2.append(round(chi2_contingency(np.array([var_1,var_0]),correction=False)[1],3))
CTS_kor = CTS.drop(["Severity","Mild","Mod","Sev"],axis=1)
zeroVar = CTS_kor.shape[1]-((VarianceThreshold(threshold=0).fit(CTS_kor)).get_support()).sum()    
##################
catDF = CTS.groupby("Severity").sum()[["Sex","Side","Diabetes","NP","Weakness"]]
sex, rside, diab, np, weak = catDF["Sex"],catDF["Side"],catDF["Diabetes"],catDF["NP"],catDF["Weakness"]
hands = CTS.groupby("Severity").count()["NP"]
handsx = hands*100

```
```{r, echo=FALSE,warning=FALSE,message=FALSE}
  means <- as.data.frame(py$means)
  stds <- as.data.frame(py$stds)
  meanOval <- as.data.frame(t(py$meanoval))
  stdOval <- as.data.frame(t(py$stdoval))
  CTS <- as.data.frame(read_csv("data/CTS.csv"))
  seed<-0923
  set.seed(seed)
  ind<-sample(2,nrow(CTS),replace = T,prob = c(0.7,0.3))
  traindata_top <- CTS[ind==1,]
  testdata_top <- CTS[ind==2,]
  # BURADAN SORNASI R VERİ ÖNİŞLEME
  CTS$Severity<-as.factor(CTS$Severity)
  CTS$Mild<-as.factor(CTS$Mild)
  CTS$Mod<-as.factor(CTS$Mod)
  CTS$Sev<-as.factor(CTS$Sev)
  CTS$Sex <-as.factor(CTS$Sex)
  CTS$Side <-as.factor(CTS$Side)
  CTS$Diabetes <-as.factor(CTS$Diabetes)
  CTS$NP <- as.factor(CTS$NP)
  CTS$Weakness <- as.factor(CTS$Weakness)
  predata<-CTS
  st_model<-preProcess(predata[,5:10], method=c("center","scale"))
  data<-predict(st_model, predata)
  data=as.data.frame(data)
  ohe_feats = c('Sex','Side','Diabetes','NP','Weakness')
  dummies = dummyVars(~ Sex+Side+Diabetes+NP+Weakness, data = data)
  df_ohe <- as.data.frame(predict(dummies, newdata = data))
  df_combined <- cbind(data[,-c(which(colnames(data) %in% ohe_feats))],df_ohe)
  dat = as.data.table(df_combined)
  traindata<-dat[ind==1,]
  testdata<-dat[ind==2,]
  trainmc<-traindata
  testmc<-testdata
  trainmc$Mild=NULL
  trainmc$Mod=NULL
  trainmc$Sev=NULL
  testmc$Mild=NULL
  testmc$Mod=NULL
  testmc$Sev=NULL
  hco <- nrow(CTS)
  hco <- hco * 100
```
```{python,echo=FALSE,warning=FALSE,message=FALSE}
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
LE = LabelEncoder().fit(["mild","moderate","severe"])
traindata_P = pd.DataFrame(r.traindata_top)
traindata_P.drop(["Mild","Mod","Sev"],axis=1,inplace=True)
testdata_P = pd.DataFrame(r.testdata_top)
testdata_P.drop(["Mild","Mod","Sev"],axis=1,inplace=True)
X_train, X_test, y_train, y_test = traindata_P.drop(["Severity"],axis=1),testdata_P.drop(["Severity"],axis=1),pd.DataFrame(LE.transform(traindata_P.Severity)),pd.DataFrame(LE.transform(testdata_P.Severity))
Stand = StandardScaler().fit(r.CTS[["Age","BMI","CSA","PB","Duration","NRS"]])
X_train[["Age","BMI","CSA","PB","Duration","NRS"]]=pd.DataFrame(Stand.transform(X_train[["Age","BMI","CSA","PB","Duration","NRS"]]),columns=["Age","BMI","CSA","PB","Duration","NRS"])
y_train = y_train.to_numpy().ravel()
X_test[["Age","BMI","CSA","PB","Duration","NRS"]] = pd.DataFrame(Stand.transform(X_test[["Age","BMI","CSA","PB","Duration","NRS"]]),columns=["Age","BMI","CSA","PB","Duration","NRS"])
y_test = y_test.to_numpy().ravel()
```  
Veri setindeki düşük varyanslı değişken sayısı : `r py$zeroVar`


|                                    | Overall |
|:-----------------------------------|--------:|
| Age,years (mean $\pm$ SD)          | `r meanOval$Age` $\pm$ `r stdOval$Age` |
| BMI, kg/m$^2$ (mean $\pm$ SD)      | `r meanOval$BMI` $\pm$ `r stdOval$BMI` |
| Duration, months (mean $\pm$ SD)   | `r meanOval$Duration` $\pm$ `r stdOval$Duration` |
| NRS (mean $\pm$ SD)                | `r meanOval$NRS` $\pm$ `r stdOval$NRS` |
| CSA, mm$^2$ (mean $\pm$ SD)        | `r meanOval$CSA` $\pm$ `r stdOval$CSA` |
| PB, mm (mean $\pm$ SD)             | `r meanOval$PB` $\pm$ `r stdOval$PB` |

Table: (\#tab:deneme) Sayısal Değişkenlerin Tanımlayıcı İstatistikleri

|                                    | Mild                                | Moderate                            | Severe                              | P Value |
|:-----------------------------------|------------------------------------:|------------------------------------:|------------------------------------:|--------:|
| Age,years (mean $\pm$ SD)          | `r means$Age[1]` $\pm$ `r stds$Age[1]` | `r means$Age[2]` $\pm$ `r stds$Age[2]` | `r means$Age[3]` $\pm$ `r stds$Age[3]` | `r py$p_values[1]`|
| BMI, kg/m$^2$ (mean $\pm$ SD)      | `r means$BMI[1]` $\pm$ `r stds$BMI[1]` | `r means$BMI[2]` $\pm$ `r stds$BMI[2]` | `r means$BMI[3]` $\pm$ `r stds$BMI[3]` | `r py$p_values[2]`|
| Duration, months (mean $\pm$ SD)   | `r means$Duration[1]` $\pm$ `r stds$Duration[1]` | `r means$Duration[2]` $\pm$ `r stds$Duration[2]` | `r means$Duration[3]` $\pm$ `r stds$Duration[3]` | `r py$p_values[3]`|
| NRS (mean $\pm$ SD)                | `r means$NRS[1]` $\pm$ `r stds$NRS[1]` | `r means$NRS[2]` $\pm$ `r stds$NRS[2]` | `r means$NRS[3]` $\pm$ `r stds$NRS[3]` | `r py$p_values[4]`|
| CSA, mm$^2$ (mean $\pm$ SD)        | `r means$CSA[1]` $\pm$ `r stds$CSA[1]` | `r means$CSA[2]` $\pm$ `r stds$CSA[2]` | `r means$CSA[3]` $\pm$ `r stds$CSA[3]` | `r py$p_values[5]`|
| PB, mm (mean $\pm$ SD)             | `r means$PB[1]` $\pm$ `r stds$PB[1]` | `r means$PB[2]` $\pm$ `r stds$PB[2]` | `r means$PB[3]` $\pm$ `r stds$PB[3]` | `r py$p_values[6]`|

Table: (\#tab:descrip) Değişkenlerin Bağımlı Değişkene Göre Tanımlayıcı İstatistikleri \tiny (P-Value Değerleri Tek Yönlü Varyans Analiz Testi ile Elde Edilmiştir.)  

\
\
\  


|                                          | Mild | Moderate | Severe | P value |
|------------------------------------------|-----:|---------:|-------:|-------:|
| Eller,  n (%)                            |`r py$hands[1]` (`r round((py$handsx[1]/hco)*100,1)`)|`r py$hands[2]` (`r round((py$handsx[2]/hco)*100,1)`)|`r py$hands[3]` (`r round((py$handsx[3]/hco)*100,1)`)|    -    |
| Cinsiyet (Kadın), n (%)                  |`r py$sex[1]` (`r round(py$sex[1]/sum(CTS["Mild"] == "mild")*100,1)`)|`r py$sex[2]` (`r round(py$sex[2]/sum(CTS["Mod"] == "moderate")*100,1)`)|`r py$sex[3]` (`r round(py$sex[3]/sum(CTS["Sev"] == "severe")*100,1)`)| `r py$p_vals2[1]`|
| Sağ El Kasılması, n (%)                  |`r py$rside[1]` (`r round(py$rside[1]/sum(CTS["Mild"] == "mild")*100,1)`)|`r py$rside[2]` (`r round(py$rside[2]/sum(CTS["Mod"] == "moderate")*100,1)`)|`r py$rside[3]` (`r round(py$rside[3]/sum(CTS["Sev"] == "severe")*100,1)`)| `r py$p_vals2[2]`|
| Diyabet, n (%)                           |`r py$diab[1]` (`r round(py$diab[1]/sum(CTS["Mild"] == "mild")*100,1)`)|`r py$diab[2]` (`r round(py$diab[2]/sum(CTS["Mod"] == "moderate")*100,1)`)|`r py$diab[3]` (`r round(py$diab[3]/sum(CTS["Sev"] == "severe")*100,1)`)| `r py$p_vals2[3]`|
| Gece Ağrıları, n (%)                     |`r py$np[1]` (`r round(py$np[1]/sum(CTS["Mild"] == "mild")*100,1)`)|`r py$np[2]` (`r round(py$np[2]/sum(CTS["Mod"] == "moderate")*100,1)`)|`r py$np[3]` (`r round(py$np[3]/sum(CTS["Sev"] == "severe")*100,1)`)| `r py$p_vals2[4]`|
| Avuç İçi Zayıflık ve/veya Körelme, n (%) |`r py$weak[1]` (`r round(py$weak[1]/sum(CTS["Mild"] == "mild")*100,1)`)|`r py$weak[2]` (`r round(py$weak[2]/sum(CTS["Mod"] == "moderate")*100,1)`)|`r py$weak[3]` (`r round(py$weak[3]/sum(CTS["Sev"] == "severe")*100,1)`)| `r py$p_vals2[5]`|

Table: (\#tab:catvar) Katagorik Değişkenlerin Bağımlı Değişkence Frekans Dağılımı \tiny (P-Value Değerleri Ki-Kare Bağımsızlık Testi ile Elde Edilmiştir.)   

```{python,echo=FALSE}
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix, ConfusionMatrixDisplay, auc
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn.preprocessing import label_binarize
def roc(model):
    """ Unfitted model"""
    model_name = str(model.__class__).split(".")[-1][:-2]
    model = OneVsRestClassifier(model).fit(X_train,y_train)
    plt.figure()
    y_pred = model.predict_proba(X_test)
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(3):
        fpr[i], tpr[i], _ = roc_curve(y_test, y_pred[:, i],pos_label=i)
        roc_auc[i] = auc(fpr[i], tpr[i])
    colors = cycle(["aqua", "darkorange", "cornflowerblue"])
    for i, color, j in zip(range(3), colors,["Mild","Moderate","Severe"]):
        plt.plot(
            fpr[i],
            tpr[i],
            color=color,
            lw=2,
            label="ROC curve of class {0} (area = {1:0.2f})".format(j, roc_auc[i])
        )
    plt.plot([0, 1], [0, 1], "k--", lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Some extension of Receiver operating characteristic to multiclass")
    plt.legend(loc="lower right")
    plt.savefig(f'figure/roc_curve_{model_name}.png')
```


## K-En Yakın Komşuluk Modeli  
Bu bölümde veri setimiz üzerinde K - En yakın komşuluk modelini kullanacak ve çıktılarını değerlendireceğiz.  

### Hiper Parametre Seçimi  
Daha önce belirlediğimiz parametre uzayını ve Scikit-Learn kütüphanesinde bulunan GridSearchCV algoritması ile en uygun doğruluk oranını yakalayana kadar çalışması sağlandı.
```{python echo=FALSE}
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,classification_report,balanced_accuracy_score
KNN_model = KNeighborsClassifier()
#params = {"n_neighbors":np.arange(5,200),
#          "weights":["uniform", "distance"],
#          "algorithm":["auto","ball_tree","kd_tree","brute"]}
#GSC = GridSearchCV(KNN_model,param_grid=params,
#                   cv=10,verbose=1,scoring="accuracy").fit(X_train,y_train)
#pd.DataFrame(GSC.cv_results_).to_csv("data/KNN_GridSearch_Results.csv",index=False)
knn = pd.read_csv("data/KNN_GridSearch_Results.csv")
plt.figure(figsize=(10,5),dpi=60);
sns.lineplot(x=knn.param_n_neighbors,y=knn.mean_test_score*100,hue=knn.param_weights);
plt.ylabel("Train Scores");
plt.xlabel("Number of Neighbors");
plt.savefig("figure/KNN_Grid_Graph.png");
```
```{r echo=FALSE,fig.align="center",fig.cap="Model Train Scores",out.width="110%",out.height="55%"}
knitr::include_graphics(path="figure/KNN_Grid_Graph.png")
```
```{python echo=FALSE}
#print(f'En İyi Parametreler : {GSC.best_params_}')
print("En İyi Parametreler:{'algorithm':'auto','n_neighbors':33,'weights':'distance'}")

```

### En İyi Parametreli Model
GridSearchCV algoritması ile bulduğumuz parametrelerle kurulan modelimizin sınıflandırma metrikleri.
```{python,echo=FALSE}
KNN_model = KNeighborsClassifier(n_neighbors = 33,
                                 weights ='distance').fit(X_train,y_train)
y_pred = KNN_model.predict(X_test)
print(classification_report(y_test,y_pred,target_names=["Mild","Moderate","Severe"]))
print(f'Balanced Accuracy Score : {balanced_accuracy_score(y_test,y_pred)}')
```
<!--```{r echo=FALSE, fig.align='center',fig.show='hold',fig.cap="K-NN Metrikleri",out.height="25%",out.width="70%"}
knitr::include_graphics(path="figure/knn_metrics.png")
```-->

```{python,echo=FALSE}
ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred),display_labels=["Mild","Moderate","Severe"]).plot();
plt.savefig("figure/knn_conf.png")
```
```{python,echo=FALSE}
roc(KNeighborsClassifier(n_neighbors = 33,
                                 weights ='distance'))
```

```{r echo=FALSE,fig.align="center",fig.cap="K-En Yakın Komşuluk Modeli Karmaşıklık Matrisi",out.width="105%",out.height="60%"}
knitr::include_graphics(path="figure/knn_conf.png")
```
```{r echo=FALSE,fig.align="center",fig.cap="K-En Yakın Komşuluk Modeli ROC Eğrisi ve AUC Değeri",out.width="105%",out.height="60%"}
knitr::include_graphics(path="figure/roc_curve_KNeighborsClassifier.png")
```
## Rassal Ormanlar Modeli  
Bu bölümde veri setimiz üzerinde Rassal Ormanlar modelini kullanacak ve çıktılarını değerlendireceğiz.  

### Hiper Parametre Seçimi  
Daha önce belirlediğimiz parametre uzayını ve Scikit-Learn kütüphanesinde bulunan GridSearchCV algoritması ile en uygun doğruluk oranını yakalayana kadar çalışması sağlandı.
```{python,out.height="10%",echo=FALSE}
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,classification_report,balanced_accuracy_score
RFC_model = RandomForestClassifier()
#param_grid = {"n_estimators":np.arange(350,1000,50),
              #"criterion":["gini","index"],
              #"max_features":["auto","sqrt","log2"],
              #"ccp_alpha":[0.01,0.05,.1,0.3,.5,.7,.9,1],
              #"max_samples":np.arange(1,X_train.shape[1],1)}
#GSC = GridSearchCV(RFC_model,param_grid=params,
                   #cv=10,verbose=1,scoring="accuracy",random_state=13).fit(X_train,y_train)
results = pd.read_csv("data/RF_GridSearch_Results.csv")
ginis = results[results["param_criterion"] == "gini"]
ginis = ginis[ginis["param_ccp_alpha"] <= 0.1]
ginis = ginis[ginis["mean_test_score"]>=0.704]
#Grafik Çizimi
plt.figure(figsize=(10,5),dpi=60);
plt.ylim(0.703*100,round(ginis.mean_test_score.unique().max()*100,2)+0.1);
sns.lineplot(y=ginis.mean_test_score*100,x=ginis.param_n_estimators,hue=ginis.param_ccp_alpha,palette=sns.color_palette(n_colors=3),err_style=None);
plt.ylabel("Train Scores");
plt.xlabel("Number of Trees in Forest");
plt.savefig("figure/RF_Grid_Graph.png");
```
```{r echo=FALSE,fig.align="center",fig.cap="Model Train Scores",out.width="110%",out.height="50%"}
knitr::include_graphics(path="figure/RF_Grid_Graph.png")
```
```{python echo=FALSE}
#print(f'En İyi Parametreler : {GSC.best_params_}')
print("En İyi Parametreler:{'ccp_alpha':0.05,'criterion':'gini','max_features':'auto',\n'max_samples':10,'n_estimators':350}")

```

### En İyi Parametreli Model
GridSearchCV algoritması ile bulduğumuz parametrelerle kurulan modelimizin sınıflandırma metrikleri.
```{python,echo=FALSE}
RFC_model = RandomForestClassifier(ccp_alpha=0.05,criterion="gini",max_features="auto",
                                   max_samples=10,n_estimators=350,random_state=13).fit(X_train,y_train)
y_pred = RFC_model.predict(X_test)
print(classification_report(y_test,y_pred,target_names=["Mild","Moderate","Severe"]))
print(f'Balanced Accuracy Score : {balanced_accuracy_score(y_test,y_pred)}')
```

```{python,echo=FALSE}
ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred),display_labels=["Mild","Moderate","Severe"]).plot();
plt.savefig("figure/rfc_conf.png")
```
```{python,echo=FALSE}
roc(RandomForestClassifier(ccp_alpha=0.01,criterion="gini",max_features="sqrt",
                                   max_samples=10,n_estimators=900,random_state=13))
```

```{r echo=FALSE,fig.align="center",fig.cap="Rassal Ormanlar Modeli Karmaşıklık Matrisi",out.width="105%",out.height="60%"}
knitr::include_graphics(path="figure/rfc_conf.png")
```
```{r echo=FALSE,fig.align="center",fig.cap="Rassal Ormanlar Modeli ROC Eğrisi ve AUC Değeri",out.width="105%",out.height="60%"}
knitr::include_graphics(path="figure/roc_curve_RandomForestClassifier.png")
```
## eXtreme Gradient Boosting (Xgboost)
